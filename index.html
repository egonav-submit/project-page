<!DOCTYPE html>
<html>
<head>
  <style>
    #results-carousel .item img {
      /* Adjust the width and height as needed */
      width: 100%; /* Responsive to container size */
      height: auto; /* Maintain aspect ratio */
      /* Scale up images */
      transform: scale(1.0);
      /* Center the image in case it overflows its container */
      margin-left: 0%; /* Adjust this value as needed */
      margin-top: 0%; /* Adjust this value as needed */
    }
  </style>
 

  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Learning humanoid navigation from human walking data with zero robot data">
  <meta property="og:title" content="EgoNav: Learning Humanoid Navigation from Human Data"/>
  <meta property="og:description" content="Learning humanoid navigation from human walking data with zero robot data"/>
  <meta property="og:url" content=""/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/banner_image.png" />
  <meta property="og:image:width" content="1700"/>
  <meta property="og:image:height" content="930"/>


  <meta name="twitter:title" content="EgoNav: Learning Humanoid Navigation from Human Data">
  <meta name="twitter:description" content="Learning humanoid navigation from human walking data with zero robot data">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Humanoid Navigation, Egocentric Trajectory Prediction, Diffusion Model, Visual Memory, Human-to-Robot Transfer">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>EgoNav: Learning Humanoid Navigation from Human Data</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">EgoNav: Learning Humanoid Navigation from Human Data</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">Anonymous Authors</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Anonymous Institution<br>Under Review</span>
            </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Paper PDF link -->
                      <span class="link-block">
                        <a href="static/pdfs/egonav260225_1819.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="#" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>

                  <!-- Data link -->
                  <span class="link-block">
                    <a href="#" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-database"></i>
                    </span>
                    <span>Data (Coming Soon)</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>





<!-- ===================================Contents=========================================== -->>





<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay muted loop height="100%" style="border-radius: 10px;">
        <source src="static/videos/teaser_vid.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        EgoNav enables a humanoid robot to navigate diverse, unseen environments by learning entirely from human walking data, with no robot data or finetuning.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Teaser figure -->
<section class="hero is-small">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="method-picture">
        <img src="static/images/Teaser2.png" alt="EgoNav Teaser" style="border-radius: 10px;"/>
      </div>
    </div>
  </div>
</section>
<!-- End teaser figure -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present EgoNav, a system that enables a humanoid robot to traverse diverse, unseen environments by learning entirely from human walking data, with no robot data or finetuning. A diffusion model predicts distributions of plausible future trajectories conditioned on past trajectory, a 360&deg; visual memory fusing color, depth, and semantics, and video features from a frozen DINOv3 backbone that capture appearance cues invisible to depth sensors. A hybrid sampling scheme achieves real-time inference in 10 denoising steps, and a receding-horizon controller selects paths from the predicted distribution. We validate EgoNav through offline evaluations, where it outperforms baselines in collision avoidance and multi-modal coverage, and through zero-shot deployment on a Unitree G1 humanoid across unseen indoor and outdoor environments. Behaviors such as waiting for doors to open, navigating around pedestrians, and avoiding glass walls emerge naturally from the learned prior. We release the dataset and trained models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Dataset</h2>
      <div class="content has-text-justified">
        <p>
          Our Egocentric Navigation Dataset comprises 300 minutes (5 hours) of human walking data covering over 25 km across diverse campus environments. The dataset contains 44 sequences, each approximately 7 minutes spanning 600 meters, recorded at 20 Hz. It encompasses various weather conditions (rain, sunny, overcast), surface textures (glass, solid, glossy, reflective, water), and environmental features (stairs, ramps, flat grounds, hills, off-road paths), alongside dynamic obstacles including pedestrians.
        </p>
        <p>
          Data is collected using an Intel RealSense T265 for SLAM-based 6-DoF localization and a RealSense D455 for aligned RGBD capture. At each timestep, the dataset provides: 6-DoF torso pose in a global frame, leg joint angles, torso linear and angular velocities, gait frequency, aligned color and depth images, semantic segmentation masks (8 classes via DINOv2 + Mask2Former), and precomputed visual memory panoramas.
        </p>
        <p>
          We are committed to open-sourcing the dataset and providing the software tools for data collection and processing, enabling others to extend the dataset or collect similar data in different environments.
        </p>
      </div>
      <div id="results-carousel" class="carousel results-carousel" style="max-width: 848px; margin: auto;">

  </div>
</div>
</div>
</section>
<!-- End image carousel --> 
 
 


<!-- Image carousel -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Method</h2>
      <div class="content has-text-justified">
        <div class="method-picture">
          <img src="static/images/Method.png" alt="Method Picture" style="max-width: 75%; display: block; margin: auto;"/>
        </div>
        <p>
          EgoNav learns a <strong>navigation prior</strong>&mdash;an embodiment-agnostic distribution of plausible future paths&mdash;serving as the missing middle layer between high-level task planning and low-level locomotion. The system is built on four design principles: <em>human-native</em> (learns from human walking data with no robot data), <em>scene-aware</em> (derives scene understanding from egocentric observations alone), <em>distributional</em> (models the full distribution of plausible paths), and <em>robot-ready</em> (deployable with real-time inference and latency compensation).
        </p>

        <p>
          The method takes in past trajectory and a short history of RGBD images. Color images are semantically labeled by DINOv2 with Mask2Former into 8 classes (ground, stair, door, wall, obstacle, movable, rough ground, unlabeled). Depth frames go through a Canny edge filter to remove stereo artifacts. All frames are projected and aligned into a 360&deg; egocentric panorama called "visual memory". Additionally, a frozen DINOv3 ViT backbone extracts video features that capture appearance cues invisible to depth (e.g., glass walls, dynamic agents). Conditioning on the visual memory, DINOv3 features, and past trajectory, a diffusion model generates distributions of future trajectories. A receding-horizon controller then selects collision-free paths from the predicted distribution for real-time robot deployment.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Visual Memory</h2>
      <div class="content has-text-justified">
        <div class="method-picture">
          <img src="static/images/Panorama.png" alt="Construction of Visual Panorama"/>
        </div>

        <p>
          Visual memory is a 360&deg; ego-perspective panoramic representation of the surroundings, with dimensions 180&times;360&times;5 (1 pixel per degree) covering RGB, depth, and semantic channels. A rolling buffer of RGBD frames is accumulated into a single point cloud in the global frame using camera intrinsic and extrinsic parameters. A distance-based filter removes far-away points, and the remaining points are reprojected into the current egocentric frame to form a coherent panorama.
        </p>

        <p>
          The visual memory is encoded by a pretrained spatial VAE into an 8&times;20&times;8 latent feature map that preserves geometric layout, then further compressed to a 64-dimensional embedding via a lightweight adapter. This entire pipeline runs on a Jetson Orin NX CPU within 30ms.
        </p>

        <p>
          Compared to a single camera image with a narrow ~90&deg; FOV, the visual memory captures stairs, side paths, and wall information that would otherwise be missed. It integrates multi-modal inputs into one compact representation, greatly improving model and storage efficiency.
        </p>

        <div class="method-picture">
          <img src="static/images/VisualMemory.png" alt="Visual Memory Picture"/>
        </div>

      </div>
  </div>
</div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Diffusion Model and Hybrid Generation</h2>
      <div class="content has-text-justified">
        <div class="method-picture">
          <img src="static/images/HybridGen.png" alt="Hybrid Generation"/>
          <p class="has-text-centered" style="font-size: 14px; color: gray; margin-top: 8px;">Hybrid generation step search: Best-of-N, Collision, and Smoothness scores for different DDIM/DDPM step combinations. The optimal configuration is 5 DDIM + 5 DDPM steps.</p>
        </div>
        <p>
          The trajectory diffusion model is a non-autoregressive 46M-parameter UNet that predicts all 100 future steps (5 seconds at 20 Hz) simultaneously. Multi-head self-attention layers between downsample/upsample blocks capture long-range dependencies. The model takes 9-channel trajectory representations (x-y-z position + ortho6d orientation) and fuses conditions&mdash;visual memory encoding, DINOv3 video features, and past trajectory&mdash;via an adapter module with classifier-free guidance (10% condition dropout during training).
        </p>
        <p>
          Standard DDPM sampling is too slow for real-time deployment, while DDIM sacrifices sample quality. Our hybrid generation scheme combines both: 5 DDIM steps quickly approximate the trajectory distribution, followed by 5 DDPM steps that recover fine-grained multi-modal details. This achieves 100&times; acceleration over full DDPM while maintaining distribution quality. With a batch size of 64, the model generates 110 trajectories/second, running at 1.7 Hz on a Jetson Thor.
        </p>
      </div>
  </div>
</div>
</section>
<!-- End image carousel -->



<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Offline Evaluation</h2>
      <div class="content has-text-justified">
        <p>
          We evaluate EgoNav using three metrics: <strong>Collision-Free Score (CFS)</strong> measuring trajectory feasibility against the visual memory point cloud, <strong>Smoothness</strong> measuring velocity and acceleration consistency, and <strong>Best-of-N (minADE-K)</strong> measuring multi-modal coverage by selecting the closest of K trajectories to ground truth.
        </p>

        <div class="method-picture">
          <img src="static/images/Metrics.png" alt="Metric Picture" style="max-width: 75%; display: block; margin: auto;"/>
        </div>

        <iframe src="table_ablation.html" frameborder="0" width="100%" height="620px" scrolling="no"></iframe>

        <p>
          Ablation studies show that each component contributes meaningfully: removing the semantic channel reduces collision avoidance by 5.1 points (the model cannot distinguish doors from walls), removing attention layers costs 2.6 points, and DINOv3 features, while showing modest offline improvement (+0.8), prove critical in real-world deployment for detecting glass walls and dynamic agents.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Real-World Deployment</h2>
      <div class="content has-text-justified">
        <p>
          We validate EgoNav through zero-shot deployment on a Unitree G1 humanoid across 37.5 minutes of autonomous operation covering 1,137 meters. The system achieves over 96% autonomous operation across static indoor, corridor, glass wall, and dynamic scene environments. Several natural behaviors emerge from the learned prior without explicit programming: the robot waits at closed doors until they open, finds gaps between pedestrians, and turns away from glass walls invisible to depth sensors.
        </p>

        <iframe src="table_baseline.html" frameborder="0" width="100%" height="300px" scrolling="no"></iframe>
      </div>
    </div>
  </div>
</section>



<!-- Youtube video -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Walk-through</h2>
      <div class="columns is-centered has-text-centered">
        <!-- <div class="column is-four-fifths"> -->
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <!-- Video hidden for anonymous review -->
            <p style="text-align:center; color:gray;">Video available after review.</p>
          </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->





<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{anonymous2026egonav,
        title={Learning Humanoid Navigation from Human Data},
        author = {Anonymous},
        journal={Under Review},
        year={2026},
}
</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-12">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  document.addEventListener('DOMContentLoaded', function() {
    // Generate the array of image filenames dynamically
    const imageFilenames = [];
    for (let i = 1; i <= 36; i++) {
      imageFilenames.push(i + '.png');
    }
  
    // The base path to the images
    const basePath = 'static/images/dataset_pic/';
  
    // Get the carousel container
    const carouselContainer = document.getElementById('results-carousel');
  
    // Iterate over the image filenames
    imageFilenames.forEach((filename, index) => {
      // Create the carousel item div
      const itemDiv = document.createElement('div');
      itemDiv.className = 'item';
  
      // Create the image tag
      const img = document.createElement('img');
      img.src = basePath + filename;
      img.alt = `Dataset example ${index + 1}`;
  
      // Create the subtitle
      const subtitle = document.createElement('h2');
      subtitle.className = 'subtitle has-text-centered';
      subtitle.textContent = `Dataset example ${index + 1}`;
  
      // Append the image and subtitle to the item div
      itemDiv.appendChild(img);
      itemDiv.appendChild(subtitle);
  
      // Append the item div to the carousel container
      carouselContainer.appendChild(itemDiv);
    });
  });
  </script> 

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->
<!-- Statcounter removed for anonymous review -->

<!-- End of Statcounter tracking code -->


    <!-- End of Statcounter Code -->
      

  </body>
  </html>
