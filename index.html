<!DOCTYPE html>
<html>
<head>
  <style>
    #results-carousel .item img {
      /* Adjust the width and height as needed */
      width: 100%; /* Responsive to container size */
      height: auto; /* Maintain aspect ratio */
      /* Scale up images */
      transform: scale(1.0);
      /* Center the image in case it overflows its container */
      margin-left: 0%; /* Adjust this value as needed */
      margin-top: 0%; /* Adjust this value as needed */
    }
  </style>
 

  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Learning humanoid navigation from human walking data with zero robot data">
  <meta property="og:title" content="EgoNav: Learning Humanoid Navigation from Human Data"/>
  <meta property="og:description" content="Learning humanoid navigation from human walking data with zero robot data"/>
  <meta property="og:url" content=""/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/banner_image.png" />
  <meta property="og:image:width" content="1700"/>
  <meta property="og:image:height" content="930"/>


  <meta name="twitter:title" content="EgoNav: Learning Humanoid Navigation from Human Data">
  <meta name="twitter:description" content="Learning humanoid navigation from human walking data with zero robot data">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Humanoid Navigation, Egocentric Trajectory Prediction, Diffusion Model, Visual Memory, Human-to-Robot Transfer">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>EgoNav: Learning Humanoid Navigation from Human Data</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">EgoNav: Learning Humanoid Navigation from Human Data</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">Anonymous Authors</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Anonymous Institution<br>Under Review</span>
            </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Paper PDF link -->
                      <span class="link-block">
                        <a href="static/pdfs/egonav260225_1819.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="#" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (After Review)</span>
                  </a>
                </span>

                  <!-- Data link -->
                  <span class="link-block">
                    <a href="#" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-database"></i>
                    </span>
                    <span>Data (After Review)</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>





<!-- ===================================Contents=========================================== -->>





<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay muted loop height="100%" style="border-radius: 10px;">
        <source src="static/videos/teaser_vid.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        EgoNav enables a humanoid robot to navigate diverse, unseen environments by learning entirely from human walking data, with no robot data or finetuning.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Teaser figure -->
<section class="hero is-small">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="method-picture">
        <img src="static/images/Teaser2.jpg" alt="EgoNav Teaser" style="border-radius: 10px;"/>
      </div>
    </div>
  </div>
</section>
<!-- End teaser figure -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present EgoNav, a system that enables a humanoid robot to traverse diverse, unseen environments by learning entirely from human walking data, with no robot data or finetuning. A diffusion model predicts distributions of plausible future trajectories conditioned on past trajectory, a 360&deg; visual memory fusing color, depth, and semantics, and video features from a frozen DINOv3 backbone that capture appearance cues invisible to depth sensors. A hybrid sampling scheme achieves real-time inference in 10 denoising steps, and a receding-horizon controller selects paths from the predicted distribution. We validate EgoNav through offline evaluations, where it outperforms baselines in collision avoidance and multi-modal coverage, and through zero-shot deployment on a Unitree G1 humanoid across unseen indoor and outdoor environments. Behaviors such as waiting for doors to open, navigating around pedestrians, and avoiding glass walls emerge naturally from the learned prior. We release the dataset and trained models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Real-World Deployment -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Real-World Deployment</h2>
      <div class="content has-text-justified">
        <p>
          Zero-shot deployment on a Unitree G1 humanoid: 37.5 minutes, 1,137 meters, over 96% autonomous across static, corridor, glass, and dynamic scenes. Emergent behaviors include waiting at doors, navigating around pedestrians, and avoiding glass walls.
        </p>

        <iframe src="table_baseline.html" frameborder="0" width="100%" height="300px" scrolling="no"></iframe>
      </div>
    </div>
  </div>
</section>

<!-- Demo Videos -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Demo Videos</h2>
      <div class="columns is-multiline">
        <div class="column is-one-third">
          <p class="has-text-centered has-text-weight-bold">Corridor</p>
          <video class="lazy-video" muted loop playsinline preload="none" style="width:100%; border-radius: 8px;">
            <source src="static/videos/web_corridor.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-one-third">
          <p class="has-text-centered has-text-weight-bold">Door Opening 1</p>
          <video class="lazy-video" muted loop playsinline preload="none" style="width:100%; border-radius: 8px;">
            <source src="static/videos/web_door1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-one-third">
          <p class="has-text-centered has-text-weight-bold">Door Opening 2</p>
          <video class="lazy-video" muted loop playsinline preload="none" style="width:100%; border-radius: 8px;">
            <source src="static/videos/web_door2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-one-third">
          <p class="has-text-centered has-text-weight-bold">Glass Wall</p>
          <video class="lazy-video" muted loop playsinline preload="none" style="width:100%; border-radius: 8px;">
            <source src="static/videos/web_glass.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-one-third">
          <p class="has-text-centered has-text-weight-bold">Kitchen (Clean)</p>
          <video class="lazy-video" muted loop playsinline preload="none" style="width:100%; border-radius: 8px;">
            <source src="static/videos/web_kitchen_clean.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-one-third">
          <p class="has-text-centered has-text-weight-bold">Kitchen (Cluttered)</p>
          <video class="lazy-video" muted loop playsinline preload="none" style="width:100%; border-radius: 8px;">
            <source src="static/videos/web_kitchen_dirty.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-one-third">
          <p class="has-text-centered has-text-weight-bold">Pedestrians</p>
          <video class="lazy-video" muted loop playsinline preload="none" style="width:100%; border-radius: 8px;">
            <source src="static/videos/web_people2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-one-third">
          <p class="has-text-centered has-text-weight-bold">Room (Cluttered 1)</p>
          <video class="lazy-video" muted loop playsinline preload="none" style="width:100%; border-radius: 8px;">
            <source src="static/videos/web_room_dirty1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-one-third">
          <p class="has-text-centered has-text-weight-bold">Room (Cluttered 2)</p>
          <video class="lazy-video" muted loop playsinline preload="none" style="width:100%; border-radius: 8px;">
            <source src="static/videos/web_room_dirty2.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End demo videos -->

<!-- Full uncut video -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Uncut - 3.5min</h2>
      <video controls preload="none" poster="static/images/uncut_cover.jpg" style="width:100%; border-radius: 8px;">
        <source src="static/videos/web_uncut.mp4" type="video/mp4">
      </video>
    </div>
  </div>
</section>
<!-- End full video -->

<!-- Image carousel -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Dataset</h2>
      <div class="content has-text-justified">
        <p>
          300 minutes (5 hours) of human walking data covering 25+ km across diverse campus environments. 44 sequences at 20 Hz, spanning varied weather, surfaces, and dynamic obstacles. Each timestep provides 6-DoF pose, RGBD images, semantic segmentation (8 classes), and precomputed visual memory panoramas. Dataset and tools will be released after review.
        </p>
      </div>
      <div id="results-carousel" class="carousel results-carousel" style="max-width: 848px; margin: auto;">

  </div>
</div>
</div>
</section>
<!-- End image carousel --> 
 
 


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Method</h2>
      <div class="content has-text-justified">
        <div class="method-picture">
          <img src="static/images/Method.jpg" alt="Method Picture" style="max-width: 75%; display: block; margin: auto;"/>
        </div>
        <p>
          EgoNav learns a <strong>navigation prior</strong>&mdash;an embodiment-agnostic distribution of plausible future paths&mdash;bridging high-level planning and low-level locomotion. The system is <em>human-native</em> (no robot data), <em>scene-aware</em> (egocentric observations only), <em>distributional</em> (multi-modal predictions), and <em>robot-ready</em> (real-time with latency compensation).
        </p>
        <p>
          Given past trajectory and RGBD history, frames are semantically labeled into 8 classes and fused into a 360&deg; egocentric panorama ("visual memory"). A frozen DINOv3 backbone captures appearance cues invisible to depth (glass walls, dynamic agents). A diffusion model generates trajectory distributions, and a receding-horizon controller selects collision-free paths for deployment.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Visual Memory</h2>
      <div class="content has-text-justified">
        <div class="method-picture">
          <img src="static/images/Panorama.jpg" alt="Construction of Visual Panorama"/>
        </div>

        <p>
          A 360&deg; panoramic representation (180&times;360&times;5) fusing RGB, depth, and semantic channels. RGBD frames are accumulated into a point cloud, filtered by distance, and reprojected into the current egocentric frame. Encoded by a pretrained spatial VAE into a 64-dim embedding, running within 30ms on a Jetson Orin NX. Compared to a single ~90&deg; FOV camera, the visual memory captures far more scene context in one compact representation.
        </p>

        <div class="method-picture">
          <img src="static/images/VisualMemory.jpg" alt="Visual Memory Picture"/>
        </div>

      </div>
  </div>
</div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Diffusion Model and Hybrid Generation</h2>
      <div class="content has-text-justified">
        <div class="method-picture">
          <img src="static/images/HybridGen.png" alt="Hybrid Generation"/>
          <p class="has-text-centered" style="font-size: 14px; color: gray; margin-top: 8px;">Hybrid generation step search: Best-of-N, Collision, and Smoothness scores for different DDIM/DDPM step combinations. The optimal configuration is 5 DDIM + 5 DDPM steps.</p>
        </div>
        <p>
          A 46M-parameter UNet predicts all 100 future steps (5 sec at 20 Hz) non-autoregressively, conditioned on visual memory, DINOv3 features, and past trajectory via classifier-free guidance. Our hybrid scheme&mdash;5 DDIM steps followed by 5 DDPM steps&mdash;achieves 100&times; acceleration over full DDPM while preserving multi-modal quality, generating 110 trajectories/sec at 1.7 Hz on a Jetson Thor.
        </p>
      </div>
  </div>
</div>
</section>
<!-- End image carousel -->



<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Offline Evaluation</h2>
      <div class="content has-text-justified">
        <p>
          Three metrics: <strong>Collision-Free Score (CFS)</strong>, <strong>Smoothness</strong>, and <strong>Best-of-N (minADE-K)</strong>.
        </p>

        <div class="method-picture">
          <img src="static/images/Metrics.png" alt="Metric Picture" style="max-width: 75%; display: block; margin: auto;"/>
        </div>

        <iframe src="table_ablation.html" frameborder="0" width="100%" height="620px" scrolling="no"></iframe>

        <p>
          Each component contributes meaningfully: semantic channel &minus;5.1 collision (can't distinguish doors from walls), attention &minus;2.6, and DINOv3 features prove critical in real-world glass/dynamic scenes despite modest offline gains.
        </p>
      </div>
    </div>
  </div>
</section>






<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{anonymous2026egonav,
        title={Learning Humanoid Navigation from Human Data},
        author = {Anonymous},
        journal={Under Review},
        year={2026},
}
</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-12">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  document.addEventListener('DOMContentLoaded', function() {
    // Generate the array of image filenames dynamically
    const imageFilenames = [];
    for (let i = 1; i <= 36; i++) {
      imageFilenames.push(i + '.jpg');
    }
  
    // The base path to the images
    const basePath = 'static/images/dataset_pic/';
  
    // Get the carousel container
    const carouselContainer = document.getElementById('results-carousel');
  
    // Iterate over the image filenames
    imageFilenames.forEach((filename, index) => {
      // Create the carousel item div
      const itemDiv = document.createElement('div');
      itemDiv.className = 'item';
  
      // Create the image tag
      const img = document.createElement('img');
      img.src = basePath + filename;
      img.alt = `Dataset example ${index + 1}`;
  
      // Create the subtitle
      const subtitle = document.createElement('h2');
      subtitle.className = 'subtitle has-text-centered';
      subtitle.textContent = `Dataset example ${index + 1}`;
  
      // Append the image and subtitle to the item div
      itemDiv.appendChild(img);
      itemDiv.appendChild(subtitle);
  
      // Append the item div to the carousel container
      carouselContainer.appendChild(itemDiv);
    });
  });

  // Lazy load demo videos with Intersection Observer
  const observer = new IntersectionObserver((entries) => {
    entries.forEach(entry => {
      if (entry.isIntersecting) {
        entry.target.play();
      } else {
        entry.target.pause();
      }
    });
  }, { threshold: 0.25 });

  document.querySelectorAll('.lazy-video').forEach(video => {
    observer.observe(video);
    video.addEventListener('mouseenter', () => video.controls = true);
    video.addEventListener('mouseleave', () => video.controls = false);
  });
  </script>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->
<!-- Statcounter removed for anonymous review -->

<!-- End of Statcounter tracking code -->


    <!-- End of Statcounter Code -->
      

  </body>
  </html>
